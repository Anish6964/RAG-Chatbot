from langchain.retrievers import AmazonKendraRetriever
from langchain.chains import ConversationalRetrievalChain
from langchain.prompts import PromptTemplate
from langchain import SagemakerEndpoint
from langchain.llms.sagemaker_endpoint import LLMContentHandler
import sys
import json
import os
from typing import Dict, List
from dotenv import load_dotenv

load_dotenv()


# The `bcolors` class defines different color codes for text output in the terminal.
class bcolors:
    HEADER = "\033[95m"
    OKBLUE = "\033[94m"
    OKCYAN = "\033[96m"
    OKGREEN = "\033[92m"
    WARNING = "\033[93m"
    FAIL = "\033[91m"
    ENDC = "\033[0m"
    BOLD = "\033[1m"
    UNDERLINE = "\033[4m"


MAX_HISTORY_LENGTH = os.environ.get("MAX_HISTORY_LENGTH", 10)


def build_chain():
    """
    The `build_chain` function defines a content handler class that transforms input prompts and output
    responses for a language model.
    :return: The `build_chain` function does not have a return statement, so it does not explicitly
    return anything.
    """
    region = os.environ["AWS_REGION"]
    kendra_index_id = os.environ["KENDRA_INDEX_ID"]
    endpoint_name = os.environ["LLAMA_2_ENDPOINT"]

    class ContentHandler(LLMContentHandler):
        content_type = "application/json"
        accepts = "application/json"

        def transform_input(self, prompt: str, model_kwargs: dict) -> bytes:
            # input_str = json.dumps({"inputs": [[{"role": "user", "content": prompt},]],
            #                         "parameters" : model_kwargs
            #                         })
            input_str = json.dumps({"inputs": prompt, "parameters": model_kwargs})
            return input_str.encode("utf-8")

        def transform_output(self, output: bytes) -> str:
          response_json = json.loads(output.read().decode("utf-8"))
          print("Full Response from SageMaker:", response_json)  # Debugging

          # Handle response as a dictionary
          if isinstance(response_json, dict) and "generated_text" in response_json:
              return response_json["generated_text"]
          else:
              return "Unexpected response format: " + str(response_json)


    content_handler = ContentHandler()

    llm = SagemakerEndpoint(
        endpoint_name=endpoint_name,
        region_name=region,
        model_kwargs={"max_new_tokens": 1500, "top_p": 0.8, "temperature": 0.6},
        endpoint_kwargs={"CustomAttributes": "accept_eula=true"},
        content_handler=content_handler,
    )

    retriever = AmazonKendraRetriever(index_id=kendra_index_id, region_name=region)

    prompt_template = """
  <s>[INST] <<SYS>>
  The AI is a knowledgeable assistant that can engage in friendly conversation and provide detailed answers from its context. For general questions, the AI responds conversationally. For questions related to the provided documents, the AI answers accurately based on the document content. If the AI does not know the answer to a question or if it's not present in the document, it says it does not know.
  {context}
  <</SYS>>
  Instruction: Provide an appropriate response to the question. For document-related questions, base your answer on the document content. For general questions, respond conversationally.
  Question: {question}
  Solution:
  [/INST]"""
    PROMPT = PromptTemplate(
        template=prompt_template,
        input_variables=["context", "question"],
    )
    condense_qa_template = """
  <s>[INST] <<SYS>>
  This AI can rephrase questions to make them standalone. It understands the context of a conversation and can modify follow-up questions to be clear and independent.

  Chat History:
  {chat_history}
  Follow Up Input: {question}
    <</SYS>>
  Standalone question:  [/INST]"""
    standalone_question_prompt = PromptTemplate.from_template(condense_qa_template)

    qa = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=retriever,
        condense_question_prompt=standalone_question_prompt,
        return_source_documents=True,
        combine_docs_chain_kwargs={"prompt": PROMPT},
        verbose=False,
    )
    return qa


def run_chain(chain, prompt: str, history=[]):
    """
    The function `run_chain` takes a chatbot chain and a prompt, and returns the response generated by
    the chatbot.

    :param chain: The "chain" parameter is a function that takes a dictionary as input and returns a
    dictionary as output. It represents a conversational model or chatbot that can generate responses
    based on the given input
    :param prompt: The prompt is a string that represents the question or statement that you want to
    pass to the chatbot model
    :type prompt: str
    :param history: The `history` parameter is a list that contains the chat history. It is used to keep
    track of the conversation between the user and the chatbot. Each element in the list represents a
    message in the conversation
    :return: The function `run_chain` returns the result of calling the `chain` function with a
    dictionary as an argument. The dictionary contains the keys "question", "chat_history", and
    "prompt". The value of the "question" key is the value of the `prompt` parameter, and the value of
    the "chat_history" key is the value of the `history` parameter.
    """
    return chain({"question": prompt, "chat_history": history})


def format_messages(messages: List[Dict[str, str]]) -> List[str]:
    """Format messages for Llama-2 chat models.

    The model only supports 'system', 'user' and 'assistant' roles, starting with 'system', then 'user' and
    alternating (u/a/u/a/u...). The last message must be from 'user'.
    """
    prompt: List[str] = []

    if messages[0]["role"] == "system":
        content = "".join(
            [
                "<<SYS>>\n",
                messages[0]["content"],
                "\n<</SYS>>\n\n",
                messages[1]["content"],
            ]
        )
        messages = [{"role": messages[1]["role"], "content": content}] + messages[2:]

    for user, answer in zip(messages[::2], messages[1::2]):
        prompt.extend(
            [
                "<s>",
                "[INST] ",
                (user["content"]).strip(),
                " [/INST] ",
                (answer["content"]).strip(),
                "</s>",
            ]
        )

    prompt.extend(["<s>", "[INST] ", (messages[-1]["content"]).strip(), " [/INST] "])

    return "".join(prompt)


def print_messages(prompt: str, response: str) -> None:
    """
    The `print_messages` function takes in a prompt and a response as input and prints them in a
    formatted manner.

    :param prompt: The `prompt` parameter is a string that represents the input message or prompt that
    you want to print
    :type prompt: str
    :param response: The `response` parameter is a string that represents the generated text that you
    want to print
    :type response: str
    """
    bold, unbold = "\033[1m", "\033[0m"
    print(
        f"{bold}> Input{unbold}\n{prompt}\n\n{bold}> Output{unbold}\n{response[0]['generated_text']}\n"
    )


if __name__ == "__main__":
    chat_history = []
    qa = build_chain()
    print(bcolors.OKBLUE + "Hello! How can I help you?" + bcolors.ENDC)
    print(
        bcolors.OKCYAN
        + "Ask a question, start a New search: or CTRL-D to exit."
        + bcolors.ENDC
    )
    print(">", end=" ", flush=True)
    for query in sys.stdin:
        if query.strip().lower().startswith("new search:"):
            query = query.strip().lower().replace("new search:", "")
            chat_history = []
        elif len(chat_history) == MAX_HISTORY_LENGTH:
            chat_history.pop(0)
        result = run_chain(qa, query, chat_history)
        chat_history.append((query, result["answer"]))
        print(bcolors.OKGREEN + result["answer"] + bcolors.ENDC)
        if "source_documents" in result:
            print(bcolors.OKGREEN + "Sources:")
            for d in result["source_documents"]:
                print(d.metadata["source"])
        print(bcolors.ENDC)
        print(
            bcolors.OKCYAN
            + "Ask a question, start a New search: or CTRL-D to exit."
            + bcolors.ENDC
        )
        print(">", end=" ", flush=True)
    print(bcolors.OKBLUE + "Bye" + bcolors.ENDC)